{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84c4d339",
   "metadata": {},
   "source": [
    "## Import Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a40824",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np  # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import os  # For interacting with the operating system\n",
    "import matplotlib.pyplot as plt  # For plotting graphs\n",
    "from sklearn.model_selection import ParameterGrid  # For generating parameter grid for hyperparameter tuning\n",
    "from sklearn.metrics import log_loss  # For calculating log loss metric\n",
    "\n",
    "# Suppress specific UserWarnings from the 'openpyxl' module to prevent cluttering the output\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='openpyxl')## Load the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f14ff7c",
   "metadata": {},
   "source": [
    "## Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfce1197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory where your files are located\n",
    "# data_dir = '.'  \n",
    "data_dir = os.path.join(os.path.pardir) \n",
    "\n",
    "# List to hold the dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Loop through the years and load the files\n",
    "for year in range(2005, 2020):\n",
    "    if year <= 2012:\n",
    "        file_path = os.path.join(data_dir, f'{year}.xls')\n",
    "    else:\n",
    "        file_path = os.path.join(data_dir, f'{year}.xlsx')\n",
    "    \n",
    "    # Load the file into a dataframe\n",
    "    df = pd.read_excel(file_path)\n",
    "    \n",
    "    # Append the dataframe to the list\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all the dataframes into one\n",
    "betting_data = pd.concat(dataframes, ignore_index=True)## Load the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76c52a",
   "metadata": {},
   "source": [
    "## Fixing Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "139ec16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_column_numeric(df, column_name):\n",
    "    # Check if the column contains only numeric values\n",
    "    return df[column_name].apply(lambda x: str(x).isnumeric()).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86144397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'WRank' is not numeric.\n",
      "\n",
      "Column 'LRank' is not numeric.\n",
      "\n",
      "Column 'EXW' is not numeric.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if columns are numeric before converting\n",
    "anomaly_column = ['WRank', 'LRank', 'EXW']\n",
    "for column in anomaly_column:\n",
    "    if is_column_numeric(betting_data, column):\n",
    "        print(f\"Column '{column}' is numeric.\\n\")\n",
    "    else:\n",
    "        print(f\"Column '{column}' is not numeric.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c52465c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_non_numeric_values(df, column_name):\n",
    "    # Function to check if a value is numeric\n",
    "    def is_numeric(value):\n",
    "        try:\n",
    "            float(value)\n",
    "            return True\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    # Apply the function to the column and filter non-numeric values\n",
    "    non_numeric_values = df[~df[column_name].apply(is_numeric)]\n",
    "\n",
    "    # Display the non-numeric values\n",
    "    print(f\"Non-numeric values in {column_name}:\")\n",
    "    print(non_numeric_values[[column_name]])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "487bf8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in Wsets:\n",
      "Empty DataFrame\n",
      "Columns: [Wsets]\n",
      "Index: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# WRank column\n",
    "find_non_numeric_values(betting_data, 'Wsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "874034e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in LRank:\n",
      "Empty DataFrame\n",
      "Columns: [LRank]\n",
      "Index: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# LRank column\n",
    "find_non_numeric_values(betting_data, 'LRank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c86bd727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-numeric values in EXW:\n",
      "        EXW\n",
      "23776  2.,3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# EXW column\n",
    "find_non_numeric_values(betting_data, 'EXW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "863a57ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert WRank and LRank to numeric, coercing errors\n",
    "betting_data['WRank'] = pd.to_numeric(betting_data['WRank'], errors='coerce')\n",
    "betting_data['LRank'] = pd.to_numeric(betting_data['LRank'], errors='coerce')\n",
    "\n",
    "# Fill NaN values with a high number\n",
    "betting_data['WRank'].fillna(100000, inplace=True)\n",
    "betting_data['LRank'].fillna(100000, inplace=True)\n",
    "\n",
    "\n",
    "# Correct the typo in row 38294, column 'EXW'\n",
    "if betting_data.at[38294, 'EXW'] == '2.,3':\n",
    "    betting_data.at[38294, 'EXW'] = '2.3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f8d932",
   "metadata": {},
   "source": [
    "## Preprocess Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7e33654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'higher_rank_won' to indicate if the higher-ranked player won the match\n",
    "betting_data['higher_rank_won'] = (betting_data['WRank'] < betting_data['LRank']).astype(int)\n",
    "\n",
    "# Calculate the points for the higher-ranked player:\n",
    "# If the higher-ranked player won, use their points ('WPts');\n",
    "# Otherwise, use the opponent's points ('LPts')\n",
    "betting_data['higher_rank_points'] = (\n",
    "    betting_data['higher_rank_won'] * betting_data['WPts'] + \n",
    "    betting_data['LPts'] * (1 - betting_data['higher_rank_won'])\n",
    ")\n",
    "\n",
    "# Calculate the points for the lower-ranked player:\n",
    "# If the higher-ranked player lost, use their points ('WPts');\n",
    "# Otherwise, use the opponent's points ('LPts')\n",
    "betting_data['lower_rank_points'] = (\n",
    "    (1 - betting_data['higher_rank_won']) * betting_data['WPts'] + \n",
    "    betting_data['LPts'] * betting_data['higher_rank_won']\n",
    ")\n",
    "\n",
    "# Fill any missing values in 'higher_rank_points' with 0 to avoid issues in further calculations\n",
    "betting_data['higher_rank_points'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill any missing values in 'lower_rank_points' with 0 to ensure consistency in the dataset\n",
    "betting_data['lower_rank_points'].fillna(0, inplace=True)\n",
    "\n",
    "# Calculate the difference in sets won between the winner and the loser\n",
    "betting_data['sets_difference'] = betting_data['Wsets'] - betting_data['Lsets']\n",
    "\n",
    "# Filter the DataFrame to include only rows where the match status is 'Completed'\n",
    "# This ensures that only fully played matches are considered in the analysis\n",
    "betting_data = betting_data.loc[betting_data['Comment'] == 'Completed']\n",
    "\n",
    "# Fill missing values in 'sets_difference' with the mean of the column\n",
    "mean_value = betting_data['sets_difference'].mean()\n",
    "betting_data['sets_difference'].fillna(mean_value, inplace=True)\n",
    "\n",
    "# Create a copy of the betting_data DataFrame, named 'all_matches_k', for further processing\n",
    "all_matches_k = betting_data.copy()\n",
    "\n",
    "# List of columns to drop from the dataset as they are not needed for the analysis\n",
    "columns_to_drop = [\n",
    "    'W1', 'L1', 'W2', 'L2', 'W3', 'L3', 'W4', 'L4', 'W5', 'L5', 'Wsets', 'Lsets', 'Comment',\n",
    "    'CBW', 'CBL', 'IWW', 'IWL', 'B365W', 'B365L', \n",
    "    'EXW', 'EXL', 'PSW', 'PSL', 'WPts', 'LPts', 'UBW', 'UBL', 'LBW', 'LBL', 'SJW', 'SJL',\n",
    "    'MaxW', 'MaxL', 'AvgW', 'AvgL'\n",
    "]\n",
    "\n",
    "# Drop the unnecessary columns from 'all_matches_k' to simplify the dataset for analysis\n",
    "all_matches_k = all_matches_k.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f51aff",
   "metadata": {},
   "source": [
    "## Linear MOV model:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a5d6b",
   "metadata": {},
   "source": [
    "## Function Definitions and Setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da0e0aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def win_probability(E_i, E_j):\n",
    "    \"\"\"\n",
    "    Calculate the probability of Player i winning against Player j based on their ratings.\n",
    "\n",
    "    Parameters:\n",
    "    E_i (float): The rating of Player i.\n",
    "    E_j (float): The rating of Player j.\n",
    "\n",
    "    Returns:\n",
    "    float: The probability of Player i winning.\n",
    "    \"\"\"\n",
    "    # Calculate and return the win probability using the Elo rating formula\n",
    "    return 1 / (1 + 10 ** ((E_j - E_i) / 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa2df02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_player_elo(player_name, surface=None, blend=False):\n",
    "    \"\"\"\n",
    "    Retrieve the Elo rating of a player. Optionally, the Elo rating can be surface-specific\n",
    "    or a blend of surface and general Elo ratings.\n",
    "\n",
    "    Parameters:\n",
    "    player_name (str): The name of the player.\n",
    "    surface (str, optional): The surface type (e.g., 'clay', 'grass', 'hard'). Defaults to None.\n",
    "    blend (bool, optional): Whether to use a blend of surface-specific and general Elo ratings. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    float: The Elo rating of the player, either surface-specific, blended, or general.\n",
    "    \"\"\"\n",
    "    # Check if surface-specific and blended Elo ratings should be used\n",
    "    if surface and blend:\n",
    "        # If the player is not in the blended surface-specific Elo dictionary, initialize with the initial Elo\n",
    "        if player_name not in surface_elo_blend[surface]:\n",
    "            surface_elo_blend[surface][player_name] = initial_elo\n",
    "        return surface_elo_blend[surface][player_name]\n",
    "    \n",
    "    # Check if surface-specific Elo ratings should be used\n",
    "    elif surface:\n",
    "        # If the player is not in the surface-specific Elo dictionary, initialize with the initial Elo\n",
    "        if player_name not in surface_elo[surface]:\n",
    "            surface_elo[surface][player_name] = initial_elo\n",
    "        return surface_elo[surface][player_name]\n",
    "    \n",
    "    # If no surface is specified, return the general Elo rating\n",
    "    if player_name not in players_elo:\n",
    "        # If the player is not in the general Elo dictionary, initialize with the initial Elo\n",
    "        players_elo[player_name] = initial_elo\n",
    "    \n",
    "    return players_elo[player_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1948c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_player_elo(player_name, elo, surface=None, blend=False):\n",
    "    \"\"\"\n",
    "    Set the Elo rating for a player. The rating can be set as surface-specific, \n",
    "    a blend of surface-specific and general, or general based on the parameters.\n",
    "\n",
    "    Parameters:\n",
    "    player_name (str): The name of the player whose Elo rating is being set.\n",
    "    elo (float): The Elo rating to assign to the player.\n",
    "    surface (str, optional): The surface type (e.g., 'clay', 'grass', 'hard'). Defaults to None.\n",
    "    blend (bool, optional): Whether to set a blend of surface-specific and general Elo ratings. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    None: The function updates the relevant Elo dictionary in place.\n",
    "    \"\"\"\n",
    "    # Set the Elo rating for the player in the blended surface-specific Elo dictionary\n",
    "    if surface and blend:\n",
    "        surface_elo_blend[surface][player_name] = elo\n",
    "    \n",
    "    # Set the Elo rating for the player in the surface-specific Elo dictionary\n",
    "    elif surface:\n",
    "        surface_elo[surface][player_name] = elo\n",
    "    \n",
    "    # Set the Elo rating for the player in the general Elo dictionary\n",
    "    else:\n",
    "        players_elo[player_name] = elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cad0d7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_margin(E_i, E_j, sigma):\n",
    "    \"\"\"\n",
    "    Calculate the expected margin of victory based on the Elo ratings of two players.\n",
    "\n",
    "    Parameters:\n",
    "    E_i (float): The Elo rating of Player i.\n",
    "    E_j (float): The Elo rating of Player j.\n",
    "    sigma (float): The scaling parameter that controls the spread of expected margins.\n",
    "\n",
    "    Returns:\n",
    "    float: The expected margin of victory for Player i over Player j.\n",
    "    \"\"\"\n",
    "    return (E_i - E_j) / sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d201a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_elo_mov(E_i, E_j, K, actual_margin, sigma):\n",
    "    \"\"\"\n",
    "    Update the Elo rating for a player based on the match's margin of victory.\n",
    "\n",
    "    Parameters:\n",
    "    E_i (float): The current Elo rating of player i.\n",
    "    E_j (float): The Elo rating of the opponent, player j.\n",
    "    K (float): The K-factor, which determines how much the Elo rating changes after a match.\n",
    "    actual_margin (int): The actual margin of victory (e.g., difference in sets won).\n",
    "    sigma (float): The scaling parameter for the expected margin.\n",
    "\n",
    "    Returns:\n",
    "    float: The updated Elo rating of player i.\n",
    "    \"\"\"\n",
    "    expected_m = expected_margin(E_i, E_j, sigma)\n",
    "    delta_E_i = K * (actual_margin - expected_m)\n",
    "    return E_i + delta_E_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4011f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_elo_and_probabilities_mov(df, K, sigma):\n",
    "    \"\"\"\n",
    "    Update the Elo ratings and win probabilities for each match in the dataset based on the MOV model.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing match data. Expected columns:\n",
    "                           - 'Winner': Name of the player who won the match.\n",
    "                           - 'Loser': Name of the player who lost the match.\n",
    "                           - 'sets_difference': The margin of victory in sets (e.g., 2-0 -> 2).\n",
    "    K (float): The K-factor, which determines how much the Elo rating changes after a match.\n",
    "    sigma (float): The scaling parameter for the expected margin.\n",
    "\n",
    "    Returns:\n",
    "    None: The DataFrame is modified in place with updated Elo ratings and probabilities.\n",
    "    \"\"\"\n",
    "    for index, match in df.iterrows():\n",
    "        winner_name, loser_name = match['Winner'], match['Loser']\n",
    "        margin_of_victory = match['sets_difference']\n",
    "        surface = match['Surface']\n",
    "     \n",
    "        # Retrieve current Elo ratings overall, for the surface, and the blended surface\n",
    "        winner_elo_overall = get_player_elo(winner_name)\n",
    "        loser_elo_overall = get_player_elo(loser_name)\n",
    "        winner_elo_surface = get_player_elo(winner_name, surface)\n",
    "        loser_elo_surface = get_player_elo(loser_name, surface)\n",
    "        winner_elo_blend = get_player_elo(winner_name, surface, blend=True)\n",
    "        loser_elo_blend = get_player_elo(loser_name, surface, blend=True)\n",
    "\n",
    "        # Store initial Elo ratings\n",
    "        df.at[index, 'winner_initial_elo_overall'] = winner_elo_overall\n",
    "        df.at[index, 'loser_initial_elo_overall'] = loser_elo_overall\n",
    "        df.at[index, 'winner_initial_elo_surface'] = winner_elo_surface\n",
    "        df.at[index, 'loser_initial_elo_surface'] = loser_elo_surface\n",
    "        df.at[index, 'winner_initial_elo_blend'] = winner_elo_blend\n",
    "        df.at[index, 'loser_initial_elo_blend'] = loser_elo_blend\n",
    "        \n",
    "        # Calculate win probabilities\n",
    "        df.at[index, 'prob_winner_overall'] = win_probability(winner_elo_overall, loser_elo_overall)\n",
    "        df.at[index, 'prob_winner_surface'] = win_probability(winner_elo_surface, loser_elo_surface)\n",
    "        df.at[index, 'prob_winner_blend'] = win_probability(winner_elo_blend, loser_elo_blend)\n",
    "        \n",
    "        # Determine match outcomes based on probability and who was expected to win (overall)\n",
    "        if match['higher_rank_won']:\n",
    "            df.at[index, 'match_outcome_overall'] = int(df.at[index, 'prob_winner_overall'] > 0.5)\n",
    "            df.at[index, 'prob_high_ranked_overall'] = df.at[index, 'prob_winner_overall']\n",
    "        else:\n",
    "            df.at[index, 'match_outcome_overall'] = int((1 - df.at[index, 'prob_winner_overall']) > 0.5)\n",
    "            df.at[index, 'prob_high_ranked_overall'] = 1 - df.at[index, 'prob_winner_overall']\n",
    "\n",
    "        # Determine match outcomes based on probability and who was expected to win (surface)\n",
    "        if match['higher_rank_won']:\n",
    "            df.at[index, 'match_outcome_surface'] = int(df.at[index, 'prob_winner_surface'] > 0.5)\n",
    "            df.at[index, 'prob_high_ranked_surface'] = df.at[index, 'prob_winner_surface']\n",
    "        else:\n",
    "            df.at[index, 'match_outcome_surface'] = int((1 - df.at[index, 'prob_winner_surface']) > 0.5)\n",
    "            df.at[index, 'prob_high_ranked_surface'] = 1 - df.at[index, 'prob_winner_surface']\n",
    "\n",
    "        # Determine match outcomes based on probability and who was expected to win (blend)\n",
    "        if match['higher_rank_won']:\n",
    "            df.at[index, 'match_outcome_blend'] = int(df.at[index, 'prob_winner_blend'] > 0.5)\n",
    "            df.at[index, 'prob_high_ranked_blend'] = df.at[index, 'prob_winner_blend']\n",
    "        else:\n",
    "            df.at[index, 'match_outcome_blend'] = int((1 - df.at[index, 'prob_winner_blend']) > 0.5)\n",
    "            df.at[index, 'prob_high_ranked_blend'] = 1 - df.at[index, 'prob_winner_blend']\n",
    "       \n",
    "        # Update Elo ratings overall\n",
    "        new_winner_elo_overall = update_elo_mov(winner_elo_overall, loser_elo_overall, K, margin_of_victory, sigma)\n",
    "        new_loser_elo_overall = update_elo_mov(loser_elo_overall, winner_elo_overall, K, -margin_of_victory, sigma)  # Negative margin for the loser\n",
    "        set_player_elo(winner_name, new_winner_elo_overall)\n",
    "        set_player_elo(loser_name, new_loser_elo_overall)\n",
    "        \n",
    "        # Update Elo ratings for the surface\n",
    "        new_winner_elo_surface = update_elo_mov(winner_elo_surface, loser_elo_surface, K, margin_of_victory, sigma)\n",
    "        new_loser_elo_surface = update_elo_mov(loser_elo_surface, winner_elo_surface, K, -margin_of_victory, sigma)  # Negative margin for the loser\n",
    "        set_player_elo(winner_name, new_winner_elo_surface, surface)\n",
    "        set_player_elo(loser_name, new_loser_elo_surface, surface)\n",
    "        \n",
    "        # Blended Elo ratings after the update\n",
    "        new_winner_elo_blend = (new_winner_elo_overall + new_winner_elo_surface) / 2\n",
    "        new_loser_elo_blend = (new_loser_elo_overall + new_loser_elo_surface) / 2\n",
    "\n",
    "        # Update the blended Elo ratings in the surface_elo_blend dictionary\n",
    "        set_player_elo(winner_name, new_winner_elo_blend, surface, blend=True)\n",
    "        set_player_elo(loser_name, new_loser_elo_blend, surface, blend=True)\n",
    "\n",
    "        # Store new Elo ratings\n",
    "        df.at[index, 'winner_new_elo_overall'] = new_winner_elo_overall\n",
    "        df.at[index, 'loser_new_elo_overall'] = new_loser_elo_overall\n",
    "        df.at[index, 'winner_new_elo_surface'] = new_winner_elo_surface\n",
    "        df.at[index, 'loser_new_elo_surface'] = new_loser_elo_surface\n",
    "        df.at[index, 'winner_new_elo_blend'] = new_winner_elo_blend\n",
    "        df.at[index, 'loser_new_elo_blend'] = new_loser_elo_blend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ab5060c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mov_model(df):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the Linear MOV model by calculating log loss, accuracy, and calibration.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): DataFrame containing the actual outcomes and predicted probabilities.\n",
    "                           Expected columns:\n",
    "                           - 'higher_rank_won': Actual outcome (1 if higher-ranked player won, 0 otherwise).\n",
    "                           - 'prob_high_ranked': Predicted probability of the higher-ranked player winning.\n",
    "                           - 'match_outcome': Actual match outcome (can be used to calculate accuracy).\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing:\n",
    "           - logloss_value (float): The log loss of the predictions.\n",
    "           - accuracy_value (float): The accuracy of the model.\n",
    "           - calibration_value (float): The calibration metric for the predictions.\n",
    "    \"\"\"\n",
    "    # Calculate log loss between the actual outcomes and predicted probabilities\n",
    "    logloss_value = log_loss(df.higher_rank_won, df.prob_high_ranked)\n",
    "    \n",
    "    # Calculate accuracy by comparing predicted match outcome with the actual outcome\n",
    "    accuracy_value = np.mean(df.match_outcome == df.higher_rank_won)\n",
    "    \n",
    "    # Calculate calibration by dividing the sum of predicted probabilities by the sum of actual outcomes\n",
    "    calibration_value = np.sum(df.prob_high_ranked) / np.sum(df.higher_rank_won)\n",
    "    \n",
    "    # Return the calculated metrics\n",
    "    return accuracy_value, calibration_value, logloss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73cc313",
   "metadata": {},
   "source": [
    "## Implementation of best parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d92a63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the initial Elo rating for all players\n",
    "initial_elo = 1500\n",
    "\n",
    "# Initialize an empty dictionary to store general Elo ratings for all players\n",
    "players_elo = {}\n",
    "\n",
    "# Initialize dictionaries to store surface-specific Elo ratings for different surfaces\n",
    "surface_elo = {\n",
    "    'Hard': {},  # Elo ratings for matches played on Hard courts\n",
    "    'Clay': {},  # Elo ratings for matches played on Clay courts\n",
    "    'Grass': {},  # Elo ratings for matches played on Grass courts\n",
    "    'Carpet': {}  # Elo ratings for matches played on Carpet courts\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store blended Elo ratings (general + surface-specific) for different surfaces\n",
    "surface_elo_blend = {\n",
    "    'Hard': {},  # Blended Elo ratings for Hard courts\n",
    "    'Clay': {},  # Blended Elo ratings for Clay courts\n",
    "    'Grass': {},  # Blended Elo ratings for Grass courts\n",
    "    'Carpet': {}  # Blended Elo ratings for Carpet courts\n",
    "}\n",
    "\n",
    "update_elo_and_probabilities_mov(all_matches_k, K=6, sigma=165)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec78a3f",
   "metadata": {},
   "source": [
    "## Split Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36afad0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_matches_k['Date'] = pd.to_datetime(all_matches_k['Date'], format='%Y-%m-%d')\n",
    "split_time = pd.to_datetime('2019-01-01', format='%Y-%m-%d')\n",
    "all_matches_k_train = all_matches_k[all_matches_k['Date'] < split_time]\n",
    "all_matches_k_validation = all_matches_k[all_matches_k['Date'] >= split_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c5285",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "517cead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data, prob_col, outcome_col, actual_col):\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(data[outcome_col] == data[actual_col])\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "    # Calculate calibration\n",
    "    calibration = np.sum(data[prob_col]) / np.sum(data[actual_col])\n",
    "    print(f'Calibration: {calibration:.4f}')\n",
    "\n",
    "    # Define log loss function\n",
    "    def logloss(actual, predictions):\n",
    "        epsilon = 1e-15\n",
    "        predictions = np.clip(predictions, epsilon, 1 - epsilon)\n",
    "        \n",
    "        log_loss_value = -(1 / len(actual)) * np.sum(\n",
    "            actual * np.log(predictions) + (1 - actual) * np.log(1 - predictions))\n",
    "        return log_loss_value\n",
    "\n",
    "    # Calculate log loss\n",
    "    log_loss_value = logloss(data[actual_col], data[prob_col])\n",
    "    print(f'Log Loss: {log_loss_value:.4f}')\n",
    "\n",
    "    return accuracy, calibration, log_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc53685b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mValidation Stats for Overall Elo:\u001b[0m\n",
      "Accuracy: 0.6319\n",
      "Calibration: 1.0024\n",
      "Log Loss: 0.6288\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mValidation Stats for Overall Elo:\\033[0m\")\n",
    "accuracy_overall, calibration_overall, log_loss_overall = calculate_metrics(\n",
    "    all_matches_k_validation, 'prob_high_ranked_overall', 'match_outcome_overall', 'higher_rank_won')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e12c7f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mValidation Stats for Surface-Specific Elo:\u001b[0m\n",
      "Accuracy: 0.6363\n",
      "Calibration: 0.9646\n",
      "Log Loss: 0.6272\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mValidation Stats for Surface-Specific Elo:\\033[0m\")\n",
    "accuracy_surface, calibration_surface, log_loss_surface = calculate_metrics(\n",
    "    all_matches_k_validation, 'prob_high_ranked_surface', 'match_outcome_surface', 'higher_rank_won')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2534239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mValidation Stats for Surface-Specific Elo - (Blend):\u001b[0m\n",
      "Accuracy: 0.6407\n",
      "Calibration: 0.9834\n",
      "Log Loss: 0.6237\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mValidation Stats for Surface-Specific Elo - (Blend):\\033[0m\") \n",
    "accuracy_surface_blend, calibration_surface_blend, log_loss_surface_blend = calculate_metrics(\n",
    "    all_matches_k_validation, 'prob_high_ranked_blend', 'match_outcome_blend', 'higher_rank_won')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e4ff2",
   "metadata": {},
   "source": [
    "## Filtering Top 50 and Top 100 Ranking players from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c36df3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = all_matches_k_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd0fec26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to filter dataset for top N players\n",
    "def filter_top_players(df, top_n):\n",
    "    df_top = df[(df['WRank'] <= top_n) | (df['LRank'] <= top_n)]\n",
    "    return df_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "54b61295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataset for top 50 and top 100 players\n",
    "df_top_50 = filter_top_players(df, 50)\n",
    "df_top_100 = filter_top_players(df, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ac0d9b",
   "metadata": {},
   "source": [
    "## Metrics - Top 50 & Top 100:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0e2d7507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMetrics - Top 50\u001b[0m\n",
      "Accuracy: 0.6591\n",
      "Calibration: 0.9944\n",
      "Log Loss: 0.6054\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mMetrics - Top 50\\033[0m\")\n",
    "accuracy_surface_blend_50, calibration_surface_blend_50, log_loss_surface_blend_50 = calculate_metrics(\n",
    "    df_top_50, 'prob_high_ranked_blend', 'match_outcome_blend', 'higher_rank_won')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03df546a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mMetrics - Top 100\u001b[0m\n",
      "Accuracy: 0.6426\n",
      "Calibration: 0.9894\n",
      "Log Loss: 0.6220\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1mMetrics - Top 100\\033[0m\")\n",
    "accuracy_surface_blend_100, calibration_surface_blend_100, log_loss_surface_blend_100 = calculate_metrics(\n",
    "    df_top_100, 'prob_high_ranked_blend', 'match_outcome_blend', 'higher_rank_won')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4041f905",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Log_Loss</th>\n",
       "      <th>Calibration</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whole Dataset</td>\n",
       "      <td>0.640669</td>\n",
       "      <td>0.623686</td>\n",
       "      <td>0.983432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Top 50</td>\n",
       "      <td>0.659117</td>\n",
       "      <td>0.605368</td>\n",
       "      <td>0.994424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Top 100</td>\n",
       "      <td>0.642560</td>\n",
       "      <td>0.622027</td>\n",
       "      <td>0.989449</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Log_Loss  Calibration\n",
       "0  Whole Dataset  0.640669  0.623686     0.983432\n",
       "1         Top 50  0.659117  0.605368     0.994424\n",
       "2        Top 100  0.642560  0.622027     0.989449"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to store the validation statistics\n",
    "validation_stats = pd.DataFrame({\n",
    "    'Model': [\n",
    "        'Whole Dataset', 'Top 50', 'Top 100'\n",
    "    ],\n",
    "    'Accuracy': [\n",
    "        accuracy_surface_blend, accuracy_surface_blend_50, accuracy_surface_blend_100\n",
    "    ],\n",
    "    'Log_Loss': [\n",
    "        log_loss_surface_blend, log_loss_surface_blend_50, log_loss_surface_blend_100\n",
    "    ],\n",
    "    'Calibration': [\n",
    "        calibration_surface_blend, calibration_surface_blend_50, calibration_surface_blend_100\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Print the validation statistics DataFrame\n",
    "validation_stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
